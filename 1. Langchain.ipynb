{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFT2z_xJSqan"
      },
      "source": [
        "# LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Base de daplicaciones como \n",
        "    -Asistentes \n",
        "    -Chatbot \n",
        "    -Sistemas de recomendacion\n",
        "\n",
        "Langchain es lo mejor por que facilita el desarrollo de apps ( desde seleccion del modelo, puesta en produccion y evaluacion)\n",
        "Lo mas importante Cadenada o chain ( son secuecnias de pasos que combina con API para realizar tareas complejas)\n",
        "Se integra con Google search y Wolfram alfa\n",
        "\n",
        "¿Que es un agente?\n",
        "El Agente PERCIBE su entorno y actua en consecuencia (asi actuan lo agentes con LLM)\n",
        "\n",
        "En la documentacion de Langchain, encontramos los provedores con los que podemos trabajar y todos los LLM con lo que podemos trabajar.\n",
        "https://python.langchain.com/v0.2/docs/integrations/llms/\n",
        "\n",
        "Con los pipeline locales accedemos a Huggingface end point https://python.langchain.com/v0.2/docs/integrations/llms/huggingface_endpoint/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3HssISYStt3"
      },
      "source": [
        "## Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUagYHe1N9-n"
      },
      "outputs": [],
      "source": [
        "#%pip install --upgrade --quiet (se borra el quiet para ver lo que se esta instalando ) huggingface_hub\n",
        "%pip install --upgrade transformers langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvL1wWcMOiWo"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10}, # La respuesta tendra 10 tokens\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHTVKv5OtkM"
      },
      "outputs": [],
      "source": [
        "question = \"What is electroencephalography?\"\n",
        "hf.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agentes Intelingentes con LangChain\n",
        "\n",
        "Los Agentes en LangChain son entidades autonomas capaces de percibir, razonar y actuar usando herramientas externa.\n",
        "\n",
        "Mira la Guia conceptual de Langchain https://python.langchain.com/docs/concepts/\n",
        "\n",
        "    -Langchain core ( bases vectoriales)\n",
        "    -Lagnchain ( contruir agente sy cadenas)\n",
        "    -Langchain community (modelos de openai, hugging face o google )\n",
        "    -Langchain Expression Language (leguaje de expresion que le pertenece a langchain para poder construir el agente)\n",
        "\n",
        "Vamos a la seccion de agentes https://python.langchain.com/docs/concepts/#agents para saber como en lang chain se contruye cada uno de los agentes ( construir un agente inteligente en lang chain)\n",
        "\n",
        "EL agente puede ir desde un chat bot o asistente, como tambien un robot, para tomar una accion (p.e. robot que toma decision respecto a lo que se encuentra midiendo)\n",
        "\n",
        "Vamos a seleccionar un LLM ( en chat models : OpenAI/ Hugginface/ Google gemini)\n",
        "    --> Tendremos Promp templetes ( Langchain templates o custom templates)\n",
        "        --> Usaremos una base vectorial para la construccion de respuestas por medio de las \"chains\" que creémos\n",
        "            ---> Vamos a la construccion del agente para añadir nuevas herramientas o cosntruir la que queremos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEhTKaHUUsgK"
      },
      "source": [
        "# Instalación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hugging face podemos conectarnos de manera directa, pero hay casos en los que necesitaremos en una API key para acceder.\n",
        "    \n",
        "    --> Vamos a usar el recurso de \"google AI\" https://python.langchain.com/v0.2/docs/integrations/llms/google_ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Unb0qYv-UuN-"
      },
      "outputs": [],
      "source": [
        "pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La API key la encuntras en la pagina anterior o aca ->https://ai.google.dev/ asi accedemos con nuestra cuenta de google y pedir tu key, esta llave es personal y unica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82X3dxvdXBX0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass('API_KEY: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo9Gmi5eauuo"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXsa3vgIbUMt"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aca vemos la guia para Open AI https://python.langchain.com/v0.2/docs/integrations/llms/openai/ y para crear tu \"key\" dentro de la pagina busca o aca -> https://platform.openai.com/docs/overview ( lo mas probable es que tengas que poner credito)\n",
        "\n",
        "    --> El unico servicio gratuito en Hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zdfGABrSbV_6"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9WUXMYLcKfB"
      },
      "source": [
        "# DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvJ6f6vpbV8C"
      },
      "outputs": [],
      "source": [
        "# Instalamos la base de datos vectorial, la informaion la vectorizamos y las vamos a guardar en DB para acceder a ellas y las chain,\n",
        "\n",
        "!pip install chromadb pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQbbCTVOi-wf"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmM2HWaabksx"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkZl4a0eilSS"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo',\n",
        "                 temperature=0,\n",
        "                 max_tokens = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [(\n",
        "    'system',\n",
        "    'Eres un profesor de programacion en python. Enseña a programar'\n",
        "    ),\n",
        "    (\n",
        "    'human', 'Quiero instalar python en windows'\n",
        "    )\n",
        "           ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_msg = llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'¡Claro! Aquí te dejo los'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ai_msg.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Crear una plantilla de prompt para traducciones\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Eres un asistente útil que traduce {input_language} a {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Encadenar la plantilla con el modelo de Google AI\n",
        "chain = prompt | llm\n",
        "\n",
        "# Invocar la cadena con parámetros específicos\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
