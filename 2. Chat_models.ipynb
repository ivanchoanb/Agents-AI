{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBA_vUmqLuY8"
      },
      "source": [
        "# Chat Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una version especializada de los modelos de lenguaje para manejar interacciones ocnversacionales\n",
        "\n",
        "Procesan mensajese de chat como entrada y generan respuestas en el mismo formato\n",
        "\n",
        "Langchain facilita la interaccion con varios provedores:\n",
        "\n",
        "    El Langchain lo mensajes de chat tienen distintos roles o estructuras:\n",
        "\n",
        "    human messages que representa mensajes enviados por el usuario,\n",
        "    AI messages que son los generados por el modelo\n",
        "    System messages : Instrucciones dadas al modelo sobre como debe comportarse\n",
        "    Fuction messages: que contien resultados de funciones externas invocadas por el modelo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uq6-i80OQnw"
      },
      "source": [
        "## Introducci√≥n a los modelos de chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdQvCnosOX9l"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVMW7wiIOX2y"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPndKXjqOXyf"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-3.5-turbo\",\n",
        "                 temperature = 0,\n",
        "                 max_tokens = 10)\n",
        "\n",
        "# Human messages, AI messages, system messages y function messages ( son los tipos de mensaje que manejaremos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qde8aI0POXr4"
      },
      "outputs": [],
      "source": [
        "messages = [(\n",
        "    \"system\",\n",
        "\n",
        "    # Rol que queremos que tome el LLM\n",
        "    \"Eres un profesor de programacion en python, Ense√±a a programar\"\n",
        "    ),\n",
        "    (\n",
        "    \"human\", \"Quiero instalar python en wondows\"\n",
        "     )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0bkJaEoOXnb"
      },
      "outputs": [],
      "source": [
        "ai_msg = llm.invoke(messages)\n",
        "# Va a crearse metadata sobre la respuesta que se esta creando y la capturamos en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsRqLenBOXhW"
      },
      "outputs": [],
      "source": [
        "#Mostrar el contenido de lo que se pidio\n",
        "ai_msg.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lecturas recomendadas para usar otros modelos distintos a Chatgpt\n",
        "\n",
        "https://platform.openai.com/docs/models\n",
        "\n",
        "Chat models | ü¶úÔ∏èüîó LangChain\n",
        "\n",
        "ChatOpenAI | ü¶úÔ∏èüîó LangChain\n",
        "\n",
        "Un chat model puede tener como entrada un string, mensaje estrcuturado o un promp para generar un chat message por lo que obtenemos informacion mas estructurada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kbugwSzOXZo"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Crear una plantilla de prompt para traducciones\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Eres un asistente √∫til que traduce {input_language} a {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Encadenar la plantilla con el modelo de Google AI\n",
        "chain = prompt | llm\n",
        "\n",
        "# Invocar la cadena con par√°metros espec√≠ficos\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn5-HdmJOS9u"
      },
      "source": [
        "## Chat Messages con OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJp4bYEeNZLi"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KheCAxfsLw7y"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcSt0J-8L-sa"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8O4supXNjzc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "messages = [ SystemMessage(content = \"Eres un asistente util __\"), # podemos modificarlo de acuerdo al problema\n",
        "            HumanMessage(content = \"Me ayudas a organizas las tareas del dia ?\"),\n",
        "            AIMessage(content= \"Claro, que tareas necesitas completar hoy ?\"),\n",
        "            HumanMessage(content = \"Tengo que enviar un correo importante, hacer ejercicio y estudiar para un examen \"),\n",
        "            AIMessage( content= \"Aqui tienen tu lista de tareas : 1. Enviar correo, 2. Hacer ejercicio, 3. Estudiar para el examen\")\n",
        "]\n",
        "\n",
        "# La parte de mensajes la hicimos manualmente, pero podemos irla actualizando a medidad que se ejecuta el modelo de chat.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NhYhh13QVKR"
      },
      "outputs": [],
      "source": [
        "response = model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvYvJRrZRGYc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "trim_messages(\n",
        "\n",
        "    messages,\n",
        "    max_tokens = 45,\n",
        "    strategy = \"last\" , # Hay mas posibilidades, me ayuda para saber de que manera y que es lo que pase al historial del modelo\n",
        "    # Por ejemplo la strategy \" last\" mantiene los ultimos mensajes en numero de tokens. \n",
        "    #   ( UTIL cuando queremos que el modelo se enfoque en la parte final de la conversacion ) \n",
        "    token_counter =  ChatOpenAI( model = \"gpt-4o\"),\n",
        "\n",
        "    include_system = True\n",
        "        # Es para saber si queremos interactuar con la parte del mensaje, y se relaciona a la strategia que estamos usandos \n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
