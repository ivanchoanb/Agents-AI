{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbDUIZ76eWQC"
      },
      "source": [
        "# Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es una estrcutura que toam un diccionario como entrada, donde cada clave  representa  una varaible que debe rellnarse en el template.\n",
        "\n",
        "Se usan para convertir la entrada del usuario en un instruccion clara para el modelo\n",
        "\n",
        "Puede ser convertido en una cadena de texto o lista de mensajes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I8nCuFbQeYfT"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "String Prompt Template: Es el tipo m√°s sencillo, donde se toma una entrada y se convierte en una cadena de texto. Se usa para crear prompts b√°sicos.\n",
        "\n",
        "    Ejemplo: Un template podr√≠a ser \"Dime un chiste sobre {topic}\". Luego, se pasa un diccionario con {\"topic\": \"gatos\"}, y la salida ser√≠a \"Dime un chiste sobre gatos\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slkgMkJhebRi"
      },
      "outputs": [],
      "source": [
        "# Como siemrpe si quieres instalarlo en un entorno especifico o en tu vs code, es tu eleccion\n",
        "\n",
        "#pip install -qU langchain-openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "crGjWomieaie"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model = \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SSrpN8AQeaTI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text='Dime un chiste gatos'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Recuerda que el modelo procesa cualquier idioma \" ver esto con otro modelos mas raros\"\n",
        "prompt_template = PromptTemplate.from_template(\"Dime un chiste {topic}\")\n",
        "\n",
        "print(prompt_template.invoke({\"topic\":\"gatos\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chat Prompt Template: Se utiliza para crear una conversaci√≥n con diferentes roles, como el sistema, el usuario, o un asistente. Permite generar listas de mensajes estructuradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jj_ji0DUi4rw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[SystemMessage(content=' Eres un asistente util', additional_kwargs={}, response_metadata={}), HumanMessage(content=' dime un chiste gatos', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \" Eres un asistente util\"),\n",
        "    (\"user\", \" dime un chiste {topic}\")\n",
        "])\n",
        "\n",
        "print(prompt_template.invoke({\"topic\":\"gatos\"}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Message Placeholder: Un lugar reservado en el template donde se puede insertar una lista de mensajes. Esto permite a√±adir m√∫ltiples mensajes en un punto espec√≠fico del template, ideal para di√°logos flexibles\n",
        "\n",
        "Invoke: M√©todo que permite pasar el diccionario de entrada al template y generar la salida final. Esto reemplaza las variables del template con los valores proporcionados. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X9oi-Xfokzgs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[SystemMessage(content=' Eres un asistente util', additional_kwargs={}, response_metadata={}), HumanMessage(content='HI', additional_kwargs={}, response_metadata={}), HumanMessage(content='Adios', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \" Eres un asistente util\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "\n",
        "    # Si quisieramos a√±adir mas informacion u otro rol, tambien lo podemos hacer\n",
        "])\n",
        "\n",
        "print(prompt_template.invoke( {\"msgs\": [ HumanMessage(content= \"HI\"), HumanMessage(content= \"Adios\")]} ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los prompt templates nos ayudan a dar instruccion al modelo y mejorar la calidad de la respuesta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrUvWBHosG3"
      },
      "source": [
        "# Few Shot prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es una tecnica que utiliza ejemplos de como debe comportarse le modelo dentro de mismo prompt, permitiendo que el modelo aprenda de ejemplo que nosotros mismos vamos a ir construyendo. ( mejora la precision y relevancia de las repuestas del modelo)\n",
        "\n",
        "Se implementa:\n",
        "\n",
        "    Usando ejemplos fijos ( donde usa en todos los casos los mismo ejemplos)\n",
        "    Usando ejemplos dinamicos ( Seleccionan dinamicamente basandose en la similitud semantica entre la entrada de usario y los ejemplos que se han venido almacenando)\n",
        "\n",
        "Aca = pueden consultarse ejemplos https://python.langchain.com/docs/how_to/few_shot_examples_chat/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oMnO0gYKqC18"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZflB7K2qDpi"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.invoke('Cuanto es : 2 ü¶ú 9').content\n",
        "#Debemos darle ejemplos para que el modelo sepa que es lo que esperamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GvspCwn7qDnH"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [{'input': '2 ü¶ú 2', 'output': '4'},\n",
        "            {'input': '2 ü¶ú 3', 'output': '5'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KBr1Xqo8qDko"
      },
      "outputs": [],
      "source": [
        "example_prompt = ChatPromptTemplate(\n",
        "    [('human', '{input}'),\n",
        "     ('ai', '{output}')]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PTde6WI5qEe8"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt = example_prompt,\n",
        "    examples = examples\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OAaEBrWlsIYY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method ChatPromptValue.to_messages of ChatPromptValue(messages=[HumanMessage(content='2 ü¶ú 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 ü¶ú 3', additional_kwargs={}, response_metadata={}), AIMessage(content='5', additional_kwargs={}, response_metadata={})])>\n"
          ]
        }
      ],
      "source": [
        "print(few_shot_prompt.invoke({}).to_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompt Template: Pasa ejemplos al modelo. Cada ejemplo tiene una entrada (input) y una salida (output) esperada, generalmente estructurados en formato de diccionario. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OfQNQ-71tqoj"
      },
      "outputs": [],
      "source": [
        "main_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \" eres un mago de las matematicas\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chains: Permite integrar m√∫ltiples pasos en una cadena, combinando el modelo con ejemplos y roles para crear interacciones m√°s complejas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Como utlizar las cadenas\n",
        "\n",
        "chain =  main_prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s3Ll3e4uodU"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"input\" : \"Cuanto es 2  ü¶ú 9\"}).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYPTIAWM3v-N"
      },
      "source": [
        "# LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LangChain Expression Language, permiten secuencias de pasos llamadas a un modelo que puede ser usado en operaciones complejas o simple ( Facilita la cracion de cadenas con soprte para streaming, ejecucion paralela, asincronia)\n",
        "\n",
        "Mas info consulta = https://python.langchain.com/docs/how_to/output_parser_structured/#lcel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DqgxJhlA3s_N"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', \"Traduce lo siguiente al {language}: \"),\n",
        "    ('human', '{text}')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "J3DhCZhd5zsh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt_template | model | parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYqV7RVi6DMa"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke({'language': 'Italian', 'text': 'hello'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0zSGxCF6ew0"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqT1kWLHnDu"
      },
      "source": [
        "# Ejercicio: Chat Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "a-m63CIgHqPR"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat_history = []\n",
        "if not chat_history:\n",
        "  system_message = SystemMessage(content='Eres un asistente √∫til')\n",
        "  chat_history.append(system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = input('Haz una pregunta: ')\n",
        "chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "response = model.invoke(chat_history).content\n",
        "chat_history.append(AIMessage(content=response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Eres un asistente √∫til'\n",
            "content='hola'\n",
            "content='¬°Hola! ¬øEn qu√© puedo ayudarte hoy?'\n",
            "content='cual es tu nombre'\n",
            "content='Soy un asistente virtual desarrollado por OpenAI, y no tengo un nombre propio. Pero estoy aqu√≠ para ayudarte con lo que necesites. ¬øEn qu√© puedo asistirte hoy?'\n",
            "content='adios'\n",
            "content='¬°Adi√≥s! Si necesitas ayuda en el futuro, no dudes en volver. ¬°Que tengas un buen d√≠a!'\n"
          ]
        }
      ],
      "source": [
        "for message in chat_history:\n",
        "  print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weSMzq4IHvma"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGDEyW9DfI4K"
      },
      "source": [
        "# Integraci√≥n de Cadenas en Proyectos : Runnable, OutputParser y Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cosulte el enlace para mas info https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykiO3iSwffpi"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ljKUjozlf9E3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "sequence = RunnableLambda(lambda x: x +1 ) | RunnableLambda(lambda x: x * 2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rpJOo1bof9AP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence.invoke(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "BYUV4NB2in3A"
      },
      "outputs": [],
      "source": [
        "sequence = RunnableLambda(lambda x: x +1 ) | {\n",
        "    'index_1' : RunnableLambda(lambda x: x * 2 ),\n",
        "    'index_2' : RunnableLambda(lambda x: x * 5 )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "V_lLVdhYk0qa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'index_1': 22, 'index_2': 55}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence.invoke(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePSGhiPVk0fC"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "87Gt40B-l2zy"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "FQTrZy5pm1eU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "sqXkKSionFFd"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-lFc_4h0nFDI"
      },
      "outputs": [],
      "source": [
        "joke_query = 'Tell me a joke'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Zec99U3bnFAj"
      },
      "outputs": [],
      "source": [
        "parser = JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "suwNTTr-nrpt"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = 'Answer the user query. \\n{format_instructions}\\n{query}',\n",
        "    input_variables = ['query'],\n",
        "    partial_variables = { 'format_instructions': parser.get_format_instructions()}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FG2Owo36o28s"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M33O9i94qyrK"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke({\"query\": joke_query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_f_D4toq7bK"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_hyqlb2sb0l"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "for s in chain.stream({\"query\": joke_query}):\n",
        "  print(s)\n",
        "  time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kegW96_5sgYw"
      },
      "outputs": [],
      "source": [
        "chunks = []\n",
        "async for chunk in model.astream(joke_query):\n",
        "  chunks.append(chunk)\n",
        "  print(chunk.content, end='', flush=True)\n",
        "  time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwkaaov6t_Oq"
      },
      "source": [
        "# Chat Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsvPWV3sCN56"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7kfzKF7CRLL"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain_openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23vvxiJnsgT2"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdB7fNSFuA6M"
      },
      "outputs": [],
      "source": [
        "model.invoke([HumanMessage(content='Hi! my name is bob')]).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UobvpwGvuA3_"
      },
      "outputs": [],
      "source": [
        "model.invoke([HumanMessage(content='What is my name?')]).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p0RB84vuA1z"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
        "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ").content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUsh4oqxuAzU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import (\n",
        "    BaseChatMessageHistory,\n",
        "    InMemoryChatMessageHistory,\n",
        ")\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntXTERMC4UVe"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "  if session_id not in store:\n",
        "    store[session_id] = InMemoryChatMessageHistory()\n",
        "  return store[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHGbQF7x4_gl"
      },
      "outputs": [],
      "source": [
        "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEBoGIwI5JlB"
      },
      "outputs": [],
      "source": [
        "config = {'configurable': {'session_id': 'abc2'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO6yHJEy5Tdc"
      },
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='Hi! I am Bob')],\n",
        "     config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghVFQpDd6hN8"
      },
      "outputs": [],
      "source": [
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC15U9uU6kS_"
      },
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='What is my name?')],\n",
        "     config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYWOlqwV6pMY"
      },
      "outputs": [],
      "source": [
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkqYPy8S6sKq"
      },
      "outputs": [],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm0PuuPR6zlU"
      },
      "outputs": [],
      "source": [
        "config = {'configurable': {'session_id': 'abc3'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcsvaXm_62zp"
      },
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='What is my name?')],\n",
        "     config=config\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f206f2XJ7vkw"
      },
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='Hi! I am Carli')],\n",
        "     config=config\n",
        ")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSRGDFJi7yQq"
      },
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='What is my name?')],\n",
        "     config=config\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TgTRPIT75-a"
      },
      "outputs": [],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Lys8Gg79QE"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            'system', 'You are a helpful assistant. Answer all question to the best of your abity.'\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name='messages')\n",
        "\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdbFOyS58B9-"
      },
      "outputs": [],
      "source": [
        "chain  = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A38PQChAMxZ"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        'messages': [HumanMessage(content='Hi! I am Bob')]\n",
        "    }\n",
        ")\n",
        "response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43qPdEOMAzaP"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        'messages': [HumanMessage(content='What is your name?')]\n",
        "    }\n",
        ")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = { 'configurable': {'session_id': 'abc5'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am a helpful assistant here to assist you with any questions or tasks you have. You can call me Assistant. How can I assist you today?'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='What is your name?')],\n",
        "    config=config\n",
        ")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello Bob! How can I assist you today?'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='Hi! I am Bob')],\n",
        "    config=config\n",
        ")\n",
        "response.content"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
